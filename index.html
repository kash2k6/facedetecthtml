<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta http-equiv="Cache-control" content="no-cache, no-store, must-revalidate">
  <meta http-equiv="Pragma" content="no-cache">
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
  <title>Face Detection</title>

  <style>
    /* Your CSS code here */
    body {
      margin: 0;
      padding: 0;
      overflow: hidden;
    }
    #liveView {
      position: relative;
      width: 100vw;
      height: 100vh;
    }
    #webcam {
      display: none; /* Hide the webcam video */
    }
    #output_canvas {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
    }
    #faceDetectionText,
    #faceCountText {
      position: absolute;
      top: 10px;
      left: 10px;
      color: white;
      font-size: 16px;
      background-color: rgba(0, 0, 0, 0.5);
      padding: 5px;
    }
    #outputDiv {
      /* Additional styles for the outputDiv if needed */
    }
  </style>
</head>

<body>
  <div id="liveView" class="videoView">
    <p id="faceDetectionText"> </p>
    <p id="faceCountText"></p>
    <video id="webcam" autoplay playsinline></video>
    <canvas id="output_canvas"></canvas>
    <div id="outputDiv"></div>
  </div>

  <!-- Include the JavaScript code -->
  <script type="module">
    import vision from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3";
    const { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;
    const faceCountElement = document.getElementById("faceCount");

    let faceLandmarker;
    let runningMode = "VIDEO";

    // Before we can use HandLandmarker class we must wait for it to finish
    // loading. Machine Learning models can be large and take a moment to
    // get everything needed to run.
    async function createFaceLandmarker() {
      const filesetResolver = await FilesetResolver.forVisionTasks(
        "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm"
      );
      faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {
        baseOptions: {
          modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task`,
          delegate: "GPU"
        },
        outputFaceBlendshapes: true,
        runningMode,
        numFaces: 2
      });
      detectWebcam();
    }
    createFaceLandmarker();

    const video = document.getElementById("webcam");
    const canvasElement = document.getElementById("output_canvas");
    const canvasCtx = canvasElement.getContext("2d");

    async function detectWebcam() {
      // Activate the webcam stream.
      navigator.mediaDevices.getUserMedia({ video: true }).then((stream) => {
        video.srcObject = stream;
        video.addEventListener("loadeddata", predictWebcam);
      });
    }

    let lastVideoTime = -1;
    let results = undefined;
    const drawingUtils = new DrawingUtils(canvasCtx);
    async function predictWebcam() {
      const radio = video.videoHeight / video.videoWidth;
      const videoWidth = 480; // Set your desired width here
      video.style.width = videoWidth + "px";
      video.style.height = videoWidth * radio + "px";
      canvasElement.style.width = videoWidth + "px";
      canvasElement.style.height = videoWidth * radio + "px";
      canvasElement.width = video.videoWidth;
      canvasElement.height = video.videoHeight;
      // Now let's start detecting the stream.
      if (runningMode === "IMAGE") {
        runningMode = "VIDEO";
        await faceLandmarker.setOptions({ runningMode: runningMode });
      }
      let startTimeMs = performance.now();
      if (lastVideoTime !== video.currentTime) {
        lastVideoTime = video.currentTime;
        results = faceLandmarker.detectForVideo(video, startTimeMs);
        const faceCount = results.faceLandmarks ? results.faceLandmarks.length : 0;
        const faceCountElement = document.getElementById("faceCountText");
        faceCountElement.textContent = `Number of faces detected: ${faceCount}`;
      }

      if (results.faceLandmarks) {
        for (const landmarks of results.faceLandmarks) {
          drawingUtils.drawConnectors(
            landmarks,
            FaceLandmarker.FACE_LANDMARKS_TESSELATION,
            { color: "#C0C0C070", lineWidth: 1 }
          );
          drawingUtils.drawConnectors(
            landmarks,
            FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,
            { color: "#FF3030" }
          );
          drawingUtils.drawConnectors(
            landmarks,
            FaceLandmarker.FACE_LANDMARKS_RIGHT_EYEBROW,
            { color: "#FF3030" }
          );
          drawingUtils.drawConnectors(
            landmarks,
            FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,
            { color: "#30FF30" }
          );
          drawingUtils.drawConnectors(
            landmarks,
            FaceLandmarker.FACE_LANDMARKS_LEFT_EYEBROW,
            { color: "#30FF30" }
          );
          drawingUtils.drawConnectors(
            landmarks,
            FaceLandmarker.FACE_LANDMARKS_FACE_OVAL,
            { color: "#E0E0E0" }
          );
          drawingUtils.drawConnectors(
            landmarks,
            FaceLandmarker.FACE_LANDMARKS_LIPS,
            { color: "#E0E0E0" }
          );
          drawingUtils.drawConnectors(
            landmarks,
            FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,
            { color: "#FF3030" }
          );
          drawingUtils.drawConnectors(
            landmarks,
            FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,
            { color: "#30FF30" }
          );
          if (results.faceLandmarks) {
            const numFaces = results.faceLandmarks.length;
            document.getElementById(
              "faceDetectionText"
            ).innerText = `Detected ${numFaces} face(s)`;
          }
        }
      }

      // Call this function again to keep predicting when the browser is ready.
      window.requestAnimationFrame(predictWebcam);
    }
  </script>
</body>

</html>
